{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb27edf3",
   "metadata": {},
   "source": [
    "# SAM / SAM‑HQ Batch Inference\n",
    "This notebook runs Segment‑Anything (SAM) or SAM‑HQ on a folder of images and saves the masks, blends, and probability maps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c3ca2",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env create -f sam.yml # Run this line once and the comment out\n",
    "!conda activate sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3ed0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from SAM.segment_anything import sam_model_registry, get_sam_label\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e449b1",
   "metadata": {},
   "source": [
    "## 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5990d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_sam_info(image,\n",
    "                 box_nms=0.7,\n",
    "                 min_mask_region_area=100,\n",
    "                 pred_iou_thresh=0.88,\n",
    "                 stability_score_thresh=0.92):\n",
    "    \"\"\"Run SAM/SAM‑HQ on a single image and return (label, blend, label_P).\"\"\"\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        label, blend, label_P = get_sam_label(\n",
    "            sam, image,\n",
    "            box_nms=box_nms,\n",
    "            min_mask_region_area=min_mask_region_area,\n",
    "            pred_iou_thresh=pred_iou_thresh,\n",
    "            stability_score_thresh=stability_score_thresh\n",
    "        )\n",
    "    return label, blend, label_P\n",
    "\n",
    "\n",
    "def process_data_img(image_dir, img_name):\n",
    "    \"\"\"Run inference on *img_name* and write outputs to disk.\"\"\"\n",
    "    png_name = os.path.splitext(img_name)[0] + '.png'\n",
    "    img      = Image.open(os.path.join(image_dir, img_name))\n",
    "\n",
    "    label, blend, label_P = get_sam_info(\n",
    "        img,\n",
    "        box_nms=box_nms,\n",
    "        min_mask_region_area=min_region,\n",
    "        pred_iou_thresh=pred_iou_thresh,\n",
    "        stability_score_thresh=stability_score_thresh,\n",
    "    )\n",
    "\n",
    "    label.save(os.path.join(lbl_dir,  png_name))\n",
    "    blend.save(os.path.join(bld_dir,  png_name))\n",
    "    np.save(os.path.join(lblp_dir, png_name.replace('.png', '.npy')), label_P)\n",
    "\n",
    "def process_data_json(image_file,img_name):\n",
    "\n",
    "    img = Image.open(os.path.join(image_file,img_name))\n",
    "    sam_info = get_sam_info(img,box_nms=box_nms,min_mask_region_area =min_region,pred_iou_thresh=pred_iou_thresh,stability_score_thresh=stability_score_thresh)\n",
    "    json_data = json.dumps(sam_info, indent=4, ensure_ascii=False)  \n",
    "\n",
    "    with open(os.path.join(sam_json_dir, img_name.split('.')[0]+'.json'),'w') as f:  \n",
    "        f.write(json_data)\n",
    "        \n",
    "def make_dirs():\n",
    "    \"\"\"Create output directories if they don't exist.\"\"\"\n",
    "    for d in (sam_img_dir, lbl_dir, bld_dir, lblp_dir, sam_json_dir):\n",
    "        os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1e6ea",
   "metadata": {},
   "source": [
    "## 3. Parameters\n",
    "Feel free to tweak these before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673428a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Inference hyper‑parameters for SAM ───────────────────────────\n",
    "min_region             = 500\n",
    "box_nms                = 0.6\n",
    "pred_iou_thresh        = 0.85\n",
    "stability_score_thresh = 0.85\n",
    "\n",
    "# ─── Model Options ───────────────────────────\n",
    "    \n",
    "'''\n",
    "\"default\": build_sam_vit_h,\n",
    "\"vit_h\": build_sam_vit_h,\n",
    "\"vit_l\": build_sam_vit_l,\n",
    "\"vit_b\": build_sam_vit_b,\n",
    "\"vit_tiny\": build_sam_vit_t\n",
    "'''\n",
    "\n",
    "# ─── Checkpoint ───────────────────────────────────────────\n",
    "sam_checkpoint = {'vit_tiny': \"SAM/ckpt/sam_hq_vit_tiny.pth\"} \n",
    "model_type     = 'vit_tiny'\n",
    "\n",
    "# ─── Paths ────────────────────────────────────────────────\n",
    "image_dir   = 'GeoSeg/data/Urban/val/images'\n",
    "\n",
    "if model_type == 'vit_tiny':\n",
    "    sam_img_dir = 'GeoSeg/data/Urban/sam_label/sam_t'  \n",
    "    sam_json_dir = 'GeoSeg/data/Urban/sam_label_info/sam_t'\n",
    "\n",
    "lbl_dir  = os.path.join(sam_img_dir, \"label\")\n",
    "bld_dir  = os.path.join(sam_img_dir, \"blend\")\n",
    "lblp_dir = os.path.join(sam_img_dir, \"label_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebfabc5",
   "metadata": {},
   "source": [
    "## 4. Load SAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60ead46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m sam \u001b[38;5;241m=\u001b[39m sam_model_registry[model_type](checkpoint\u001b[38;5;241m=\u001b[39msam_checkpoint[model_type])\n\u001b[1;32m      3\u001b[0m sam\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint[model_type])\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "print(\"Model loaded on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ea03e",
   "metadata": {},
   "source": [
    "## 5. Gen SAM masks inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a44bbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 677/677 [3:14:22<00:00, 17.23s/it]  \n"
     ]
    }
   ],
   "source": [
    "make_dirs()\n",
    "\n",
    "val_lines = os.listdir(image_dir)\n",
    "for fname in tqdm(val_lines, desc=\"Processing\"):\n",
    "    process_data_img(image_dir, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48d1b7",
   "metadata": {},
   "source": [
    "## 6. Gen SAM json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8c5884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/677 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m val_lines \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(image_dir)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_line \u001b[38;5;129;01min\u001b[39;00m tqdm(val_lines):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mprocess_data_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_line\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 39\u001b[0m, in \u001b[0;36mprocess_data_json\u001b[0;34m(image_file, img_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_data_json\u001b[39m(image_file,img_name):\n\u001b[1;32m     38\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_file,img_name))\n\u001b[0;32m---> 39\u001b[0m     sam_info \u001b[38;5;241m=\u001b[39m \u001b[43mget_sam_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbox_nms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox_nms\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_mask_region_area\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_region\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpred_iou_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_iou_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstability_score_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstability_score_thresh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(sam_info, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sam_json_dir, img_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \n",
      "File \u001b[0;32m~/.conda/envs/sam/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m, in \u001b[0;36mget_sam_info\u001b[0;34m(image, box_nms, min_mask_region_area, pred_iou_thresh, stability_score_thresh)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run SAM/SAM‑HQ on a single image and return (label, blend, label_P).\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[0;32m----> 9\u001b[0m     label, blend, label_P \u001b[38;5;241m=\u001b[39m \u001b[43mget_sam_label\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43msam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbox_nms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox_nms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_mask_region_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_mask_region_area\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_iou_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_iou_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstability_score_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstability_score_thresh\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m label, blend, label_P\n",
      "File \u001b[0;32m~/abc_sam_sessrs/SAM/segment_anything/interactive_idino_m2m_auto.py:26\u001b[0m, in \u001b[0;36mget_sam_label\u001b[0;34m(model, image, box_nms, min_mask_region_area, pred_iou_thresh, stability_score_thresh)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# image_array_copy = torch.from_numpy(image_array.copy()).permute(2,0,1).cuda()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m mask_generator \u001b[38;5;241m=\u001b[39m SamAutomaticMaskGenerator(model,\n\u001b[1;32m     16\u001b[0m         points_per_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     17\u001b[0m         points_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m         min_mask_region_area\u001b[38;5;241m=\u001b[39mmin_mask_region_area,\n\u001b[1;32m     24\u001b[0m     )\n\u001b[0;32m---> 26\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m label_RGB,label_P \u001b[38;5;241m=\u001b[39m show_demo(size,mask)\n\u001b[1;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(label_RGB\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/sam/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/abc_sam_sessrs/SAM/segment_anything/automatic_mask_generator.py:180\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image, multimask_output)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_mask_region_area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/abc_sam_sessrs/SAM/segment_anything/automatic_mask_generator.py:284\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image, multimask_output)\u001b[0m\n\u001b[1;32m    282\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 284\u001b[0m     crop_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(crop_data)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_preds\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/abc_sam_sessrs/SAM/segment_anything/automatic_mask_generator.py:338\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size, multimask_output)\u001b[0m\n\u001b[1;32m    336\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (points,) \u001b[38;5;129;01min\u001b[39;00m batch_iterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints_per_batch, points_for_image):\n\u001b[0;32m--> 338\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_im_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(batch_data)\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m batch_data\n",
      "File \u001b[0;32m~/abc_sam_sessrs/SAM/segment_anything/automatic_mask_generator.py:373\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_batch\u001b[0;34m(self, points, im_size, crop_box, orig_size, multimask_output)\u001b[0m\n\u001b[1;32m    371\u001b[0m in_points \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(transformed_points, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    372\u001b[0m in_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(in_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint, device\u001b[38;5;241m=\u001b[39min_points\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 373\u001b[0m masks, iou_preds, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_points\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimask_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Serialize predictions and store in MaskData\u001b[39;00m\n\u001b[1;32m    381\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData(\n\u001b[1;32m    382\u001b[0m     masks\u001b[38;5;241m=\u001b[39mmasks\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    383\u001b[0m     iou_preds\u001b[38;5;241m=\u001b[39miou_preds\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    384\u001b[0m     points\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mas_tensor(points\u001b[38;5;241m.\u001b[39mrepeat(masks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m    385\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/sam/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/abc_sam_sessrs/SAM/segment_anything/predictor.py:235\u001b[0m, in \u001b[0;36mSamPredictor.predict_torch\u001b[0;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits, hq_token_only)\u001b[0m\n\u001b[1;32m    228\u001b[0m sparse_embeddings, dense_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprompt_encoder(\n\u001b[1;32m    229\u001b[0m     points\u001b[38;5;241m=\u001b[39mpoints,\n\u001b[1;32m    230\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mboxes,\n\u001b[1;32m    231\u001b[0m     masks\u001b[38;5;241m=\u001b[39mmask_input,\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Predict masks\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m low_res_masks, iou_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dense_pe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimask_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhq_token_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhq_token_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterm_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterm_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Upscale the masks to the original image resolution\u001b[39;00m\n\u001b[1;32m    246\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpostprocess_masks(low_res_masks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_size)\n",
      "File \u001b[0;32m~/.conda/envs/sam/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/abc_sam_sessrs/SAM/segment_anything/modeling/mask_decoder_hq.py:143\u001b[0m, in \u001b[0;36mMaskDecoderHQ.forward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output, hq_token_only, interm_embeddings)\u001b[0m\n\u001b[1;32m    141\u001b[0m     iou_pred \u001b[38;5;241m=\u001b[39m iou_pred\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m     masks_multi \u001b[38;5;241m=\u001b[39m masks[:, mask_slice, :, :]\n\u001b[0;32m--> 143\u001b[0m     masks_sam \u001b[38;5;241m=\u001b[39m \u001b[43mmasks_multi\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks_multi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_iou_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# singale mask output, default\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     mask_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(sam_json_dir,exist_ok=True)\n",
    "\n",
    "val_lines = os.listdir(image_dir)\n",
    "for val_line in tqdm(val_lines):\n",
    "    process_data_json(image_dir, val_line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
